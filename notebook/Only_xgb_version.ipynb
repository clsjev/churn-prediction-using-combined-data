{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770550c-ad23-4349-b9e0-0cf3bd061749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 3941\n",
      "churn: 674 (17.10%)\n",
      "not churn: -4615 (-117.10%)\n",
      "all features: 10\n",
      "num: 8\n",
      "cat: 2\n",
      "Label encoding on (2 features)\n",
      "scaling(8features)\n",
      "train: 3152\n",
      "churn: 539 (17.10%)\n",
      "not: 2613 (82.90%)\n",
      "test: 789\n",
      "churn: 135 (17.11%)\n",
      "not: 654 (82.89%)\n",
      "[0]\tvalidation_0-logloss:0.44709\tvalidation_1-logloss:0.45110\n",
      "[10]\tvalidation_0-logloss:0.33093\tvalidation_1-logloss:0.34857\n",
      "[20]\tvalidation_0-logloss:0.27273\tvalidation_1-logloss:0.29741\n",
      "[30]\tvalidation_0-logloss:0.24035\tvalidation_1-logloss:0.27251\n",
      "[40]\tvalidation_0-logloss:0.21851\tvalidation_1-logloss:0.25584\n",
      "[50]\tvalidation_0-logloss:0.20141\tvalidation_1-logloss:0.24623\n",
      "[60]\tvalidation_0-logloss:0.18994\tvalidation_1-logloss:0.24019\n",
      "[70]\tvalidation_0-logloss:0.18132\tvalidation_1-logloss:0.23488\n",
      "[80]\tvalidation_0-logloss:0.17179\tvalidation_1-logloss:0.22986\n",
      "[90]\tvalidation_0-logloss:0.16489\tvalidation_1-logloss:0.22563\n",
      "[100]\tvalidation_0-logloss:0.15788\tvalidation_1-logloss:0.22220\n",
      "[110]\tvalidation_0-logloss:0.15165\tvalidation_1-logloss:0.21793\n",
      "[120]\tvalidation_0-logloss:0.14668\tvalidation_1-logloss:0.21461\n",
      "[130]\tvalidation_0-logloss:0.14133\tvalidation_1-logloss:0.21070\n",
      "[140]\tvalidation_0-logloss:0.13706\tvalidation_1-logloss:0.20911\n",
      "[150]\tvalidation_0-logloss:0.13203\tvalidation_1-logloss:0.20680\n",
      "[160]\tvalidation_0-logloss:0.12706\tvalidation_1-logloss:0.20405\n",
      "[170]\tvalidation_0-logloss:0.12339\tvalidation_1-logloss:0.20314\n",
      "[180]\tvalidation_0-logloss:0.11917\tvalidation_1-logloss:0.20015\n",
      "[190]\tvalidation_0-logloss:0.11511\tvalidation_1-logloss:0.19948\n",
      "[199]\tvalidation_0-logloss:0.11208\tvalidation_1-logloss:0.19875\n",
      " Train Accu: 0.9670\n",
      " Prec: 0.9430\n",
      " Recall: 0.8590\n",
      " F1: 0.8990\n",
      " AUC:   0.9935\n",
      " Test Accu: 0.9202\n",
      " Prec: 0.8396\n",
      " Recall: 0.6593\n",
      " F1: 0.7386\n",
      " AUC:   0.9506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   not churn       0.93      0.97      0.95       654\n",
      "       churn       0.84      0.66      0.74       135\n",
      "\n",
      "    accuracy                           0.92       789\n",
      "   macro avg       0.89      0.82      0.85       789\n",
      "weighted avg       0.92      0.92      0.92       789\n",
      "\n",
      "\n",
      " confusion_matrix:\n",
      "                 Pred not churn  Pred churm\n",
      "Real not churn     637        17\n",
      "Real churn          46        89\n",
      "\n",
      "Top 20 features:\n",
      "                 feature  importance\n",
      "                  Tenure    0.275428\n",
      "                Complain    0.159505\n",
      "        PreferedOrderCat    0.093283\n",
      "           MaritalStatus    0.084586\n",
      "         NumberOfAddress    0.077034\n",
      "       DaySinceLastOrder    0.072826\n",
      "       SatisfactionScore    0.065486\n",
      "          CashbackAmount    0.062410\n",
      "NumberOfDeviceRegistered    0.059111\n",
      "         WarehouseToHome    0.050333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "with open(\"../model/params.yaml\", \"r\") as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "data_dir = Path(params['data_dir'])\n",
    "model_dir = Path(params['model_dir'])\n",
    "\n",
    "\n",
    "df_all = pd.read_csv(data_dir / \"train.csv\")\n",
    "\n",
    "print(f\"total: {len(df_all)}\")\n",
    "print(f\"churn: {df_all['churn'].sum()} ({df_all['churn'].mean():.2%})\")\n",
    "print(f\"not churn: {(~df_all['churn']).sum()} ({(~df_all['churn']).mean():.2%})\")\n",
    "\n",
    "all_columns = [col for col in df_all.columns if col != 'churn']\n",
    "numeric_columns = df_all[all_columns].select_dtypes(\n",
    "    include=['int64', 'float64']\n",
    ").columns.tolist()\n",
    "categorical_columns = [col for col in all_columns if col not in numeric_columns]\n",
    "\n",
    "print(f\"all features: {len(all_columns)}\")\n",
    "print(f\"num: {len(numeric_columns)}\")\n",
    "print(f\"cat: {len(categorical_columns)}\")\n",
    "\n",
    "X = df_all[all_columns].copy()\n",
    "y = df_all['churn'].values\n",
    "\n",
    "# Label Encoding for categorical features\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"Label encoding on ({len(categorical_columns)} features)\")\n",
    "\n",
    "# Imputation for missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X = pd.DataFrame(X_imputed, columns=all_columns)\n",
    "\n",
    "# Scaling for numeric features\n",
    "scaler = StandardScaler()\n",
    "X[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n",
    "\n",
    "print(f\"scaling({len(numeric_columns)}features)\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y \n",
    ")\n",
    "\n",
    "print(f\"train: {len(X_train)}\")\n",
    "print(f\"churn: {y_train.sum()} ({y_train.mean():.2%})\")\n",
    "print(f\"not: {len(y_train) - y_train.sum()} ({(1-y_train.mean()):.2%})\")\n",
    "\n",
    "print(f\"test: {len(X_test)}\")\n",
    "print(f\"churn: {y_test.sum()} ({y_test.mean():.2%})\")\n",
    "print(f\"not: {len(y_test) - y_test.sum()} ({(1-y_test.mean()):.2%})\")\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=20,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_train_proba = xgb_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "y_test_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "print(f\" Train Accu: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\" Prec: {precision_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\" Recall: {recall_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\" F1: {f1_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\" AUC:   {roc_auc_score(y_train, y_train_proba):.4f}\")\n",
    "\n",
    "print(f\" Test Accu: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\" Prec: {precision_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\" Recall: {recall_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\" F1: {f1_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\" AUC:   {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
    "\n",
    "print(classification_report(y_test, y_test_pred, \n",
    "                          target_names=['not churn', 'churn']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\n confusion_matrix:\")\n",
    "print(f\"                 Pred not churn  Pred churm\")\n",
    "print(f\"Real not churn  {cm[0,0]:6d}    {cm[0,1]:6d}\")\n",
    "print(f\"Real churn      {cm[1,0]:6d}    {cm[1,1]:6d}\")\n",
    "\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': all_columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 features:\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "feature_importance.to_csv(\n",
    "    model_dir / 'standalone_xgb_feature_importance.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Most Important Features (Standalone XGBoost)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_dir / 'standalone_xgb_feature_importance.png', dpi=150)\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "xgb_for_cv = clone(xgb_model)\n",
    "xgb_for_cv.set_params(early_stopping_rounds=None)\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    xgb_for_cv,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\" cross val AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "print(f\" AUC: {[f'{s:.4f}' for s in cv_scores]}\")\n",
    "\n",
    "\n",
    "joblib.dump(xgb_model, model_dir / 'standalone_xgb.joblib')\n",
    "\n",
    "joblib.dump(label_encoders, model_dir / 'standalone_label_encoders.joblib')\n",
    "joblib.dump(scaler, model_dir / 'standalone_scaler.joblib')\n",
    "joblib.dump(imputer, model_dir / 'standalone_imputer.joblib')\n",
    "joblib.dump(all_columns, model_dir / 'standalone_columns.joblib')\n",
    "joblib.dump(numeric_columns, model_dir / 'standalone_numeric_columns.joblib')\n",
    "joblib.dump(categorical_columns, model_dir / 'standalone_categorical_columns.joblib')\n",
    "\n",
    "test_data = {\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test,\n",
    "    'y_pred': y_test_pred,\n",
    "    'y_proba': y_test_proba\n",
    "}\n",
    "joblib.dump(test_data, model_dir / 'standalone_test_results.joblib')\n",
    "print(\"results saved for comparison\")\n",
    "\n",
    "print(f\"\"\"Saved:\n",
    "  - standalone_xgb.joblib\n",
    "  - standalone_*_encoders/scaler/imputer.joblib\n",
    "  - standalone_xgb_feature_importance.csv\n",
    "  - standalone_test_results.joblib\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba522a02-9b04-4ff1-bd96-af6010eede5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
